{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde5c294",
   "metadata": {},
   "source": [
    "# Viral Text Classification using Machine Learning\n",
    "Main goals: try three machine learning models, for binary classification & for continuous (retweet) prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f5f069d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6544fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/tweet_virality.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2c17b972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>creation_time</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>followers</th>\n",
       "      <th>lists</th>\n",
       "      <th>viral</th>\n",
       "      <th>virality</th>\n",
       "      <th>confidence</th>\n",
       "      <th>...</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-23 07:30:12+00:00</td>\n",
       "      <td>goannaburrows</td>\n",
       "      <td>The VC is \"pleased to advise\" that next year t...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>566</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.503691</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-16 00:18:55+00:00</td>\n",
       "      <td>goannaburrows</td>\n",
       "      <td>Reminder: Feedback on the proposal for a natio...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>566</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.277627</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-15 07:47:58+00:00</td>\n",
       "      <td>goannaburrows</td>\n",
       "      <td>Yesterday I had an embarrassing moment that I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>566</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547355</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-14 07:28:49+00:00</td>\n",
       "      <td>goannaburrows</td>\n",
       "      <td>Today a news editor told me that 'biodiversity...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>566</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272664</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-13 03:40:24+00:00</td>\n",
       "      <td>goannaburrows</td>\n",
       "      <td>Writing productivity tip: type in a heinous fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>566</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.412515</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17874</th>\n",
       "      <td>2021-12-08 09:25:46+00:00</td>\n",
       "      <td>Sys_innovation</td>\n",
       "      <td>Very interesting this idea by Adrian Bejan htt...</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>15528</td>\n",
       "      <td>373</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.597716</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17875</th>\n",
       "      <td>2021-12-06 10:33:15+00:00</td>\n",
       "      <td>Sys_innovation</td>\n",
       "      <td>The iceberg model can be applied to any situat...</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>15528</td>\n",
       "      <td>373</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.403950</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17876</th>\n",
       "      <td>2021-12-04 11:33:49+00:00</td>\n",
       "      <td>Sys_innovation</td>\n",
       "      <td>The iceberg model for thinking about the diffe...</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>15528</td>\n",
       "      <td>373</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.482952</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17877</th>\n",
       "      <td>2020-06-16 20:54:09+00:00</td>\n",
       "      <td>WasAlmostSlater</td>\n",
       "      <td>I will not be tricked into getting excited for...</td>\n",
       "      <td>6</td>\n",
       "      <td>97</td>\n",
       "      <td>419</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.403792</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17878</th>\n",
       "      <td>2020-05-25 18:58:43+00:00</td>\n",
       "      <td>WasAlmostSlater</td>\n",
       "      <td>How did Robert Pattinson become the person I w...</td>\n",
       "      <td>39</td>\n",
       "      <td>221</td>\n",
       "      <td>419</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005893</td>\n",
       "      <td>0.324748</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17879 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   creation_time             user  \\\n",
       "0      2022-09-23 07:30:12+00:00    goannaburrows   \n",
       "1      2022-09-16 00:18:55+00:00    goannaburrows   \n",
       "2      2022-09-15 07:47:58+00:00    goannaburrows   \n",
       "3      2022-09-14 07:28:49+00:00    goannaburrows   \n",
       "4      2022-09-13 03:40:24+00:00    goannaburrows   \n",
       "...                          ...              ...   \n",
       "17874  2021-12-08 09:25:46+00:00   Sys_innovation   \n",
       "17875  2021-12-06 10:33:15+00:00   Sys_innovation   \n",
       "17876  2021-12-04 11:33:49+00:00   Sys_innovation   \n",
       "17877  2020-06-16 20:54:09+00:00  WasAlmostSlater   \n",
       "17878  2020-05-25 18:58:43+00:00  WasAlmostSlater   \n",
       "\n",
       "                                                   tweet  retweets  favorites  \\\n",
       "0      The VC is \"pleased to advise\" that next year t...         0          7   \n",
       "1      Reminder: Feedback on the proposal for a natio...         1          1   \n",
       "2      Yesterday I had an embarrassing moment that I ...         0          5   \n",
       "3      Today a news editor told me that 'biodiversity...         0          8   \n",
       "4      Writing productivity tip: type in a heinous fo...         0          8   \n",
       "...                                                  ...       ...        ...   \n",
       "17874  Very interesting this idea by Adrian Bejan htt...        10         22   \n",
       "17875  The iceberg model can be applied to any situat...         9         30   \n",
       "17876  The iceberg model for thinking about the diffe...         8         22   \n",
       "17877  I will not be tricked into getting excited for...         6         97   \n",
       "17878  How did Robert Pattinson become the person I w...        39        221   \n",
       "\n",
       "       followers  lists  viral  virality  confidence  ...  89  90  91  92  93  \\\n",
       "0            566      7      0  0.000000    0.503691  ...   0   0   0   0   0   \n",
       "1            566      7      0  0.000112    0.277627  ...   0   0   0   0   0   \n",
       "2            566      7      0  0.000000    0.547355  ...   0   0   0   0   0   \n",
       "3            566      7      0  0.000000    0.272664  ...   0   0   0   0   0   \n",
       "4            566      7      0  0.000000    0.412515  ...   0   0   0   0   0   \n",
       "...          ...    ...    ...       ...         ...  ...  ..  ..  ..  ..  ..   \n",
       "17874      15528    373      1  0.000041    0.597716  ...   0   0   0   0   0   \n",
       "17875      15528    373      1  0.000037    0.403950  ...   0   0   0   0   0   \n",
       "17876      15528    373      1  0.000033    0.482952  ...   0   0   0   0   0   \n",
       "17877        419      9      1  0.000907    0.403792  ...   0   0   0   0   0   \n",
       "17878        419      9      1  0.005893    0.324748  ...   0   0   0   0   0   \n",
       "\n",
       "       94  95  96  97  98  \n",
       "0       0   0   0   0   0  \n",
       "1       0   0   0   0   0  \n",
       "2       0   0   0   0   0  \n",
       "3       0   0   0   0   0  \n",
       "4       0   0   0   0   0  \n",
       "...    ..  ..  ..  ..  ..  \n",
       "17874   0   0   0   0   0  \n",
       "17875   0   0   0   0   0  \n",
       "17876   0   0   0   0   0  \n",
       "17877   0   0   0   0   0  \n",
       "17878   0   0   0   0   0  \n",
       "\n",
       "[17879 rows x 109 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "78ed938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning X and Y Values\n",
    "X = data.drop(['virality', 'retweets', 'favorites', 'viral', 'creation_time', 'user', 'tweet', 'followers', 'lists'], axis=1) # Troubleshooting []\n",
    "y = data['viral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4109e718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "08e96294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class imbalance in the dataset is considerable, with a target constituting only 19.9% of the data.\n"
     ]
    }
   ],
   "source": [
    "# Illustrating the Class Imbalance\n",
    "plt.show(sns.countplot(x=data['viral']))\n",
    "class_imbalance = round(sum(data['viral']) / len(data['viral']), 3) * 100\n",
    "print(f'The class imbalance in the dataset is considerable, with a target constituting only {round(class_imbalance, 3)}% of the data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "942595b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Imbalance addresssed with SMOTENC:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Balancing class-imbalanced data with SMOTENC\n",
    "sm = SMOTENC(categorical_features=[98], random_state=42)\n",
    "X_train_SMOTE, y_train_SMOTE = sm.fit_resample(X_train, y_train.ravel())\n",
    "print('Class Imbalance addresssed with SMOTENC:')\n",
    "sns.countplot(x=y_train_SMOTE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c0f3af11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 65.07%\n",
      "Validation accuracy: 62.04%\n"
     ]
    }
   ],
   "source": [
    "log_reg=LogisticRegression()\n",
    "log_reg.fit(X_train_SMOTE,y_train_SMOTE)\n",
    "\n",
    "print('Training Accuracy: {:.4}%'.format(log_reg.score(X_train_SMOTE, y_train_SMOTE) * 100))\n",
    "print('Validation accuracy: {:.4}%'.format(log_reg.score(X_test, y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "656d406b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 73.74%\n",
      "Validation accuracy: 63.91%\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and Fit an XGBClassifier\n",
    "XGB = XGBClassifier()\n",
    "XGB.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "\n",
    "# Accuracy of training and test sets\n",
    "print('Training Accuracy: {:.4}%'.format(XGB.score(X_train_SMOTE, y_train_SMOTE) * 100))\n",
    "print('Validation accuracy: {:.4}%'.format(XGB.score(X_test, y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3ddc51fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 99.91%\n",
      "Validation accuracy: 62.01%\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and fit a RandomForestClassifier\n",
    "forest = RandomForestClassifier()\n",
    "forest.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "\n",
    "# Training and Testing accuracy score\n",
    "print('Training Accuracy: {:.4}%'.format(forest.score(X_train_SMOTE, y_train_SMOTE) * 100))\n",
    "print('Validation accuracy: {:.4}%'.format(forest.score(X_test, y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fc066c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg Validation set Accuracy:  62.04%\n",
      "XGBoost Validation set Accuracy: 63.91%\n",
      "Forest Validation set Accuracy:  62.01%\n",
      "\n",
      "Logistic Regression Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.61      0.72      3582\n",
      "           1       0.30      0.68      0.42       888\n",
      "\n",
      "    accuracy                           0.62      4470\n",
      "   macro avg       0.59      0.64      0.57      4470\n",
      "weighted avg       0.77      0.62      0.66      4470\n",
      "\n",
      "\n",
      "XGBoost Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.65      0.74      3582\n",
      "           1       0.30      0.59      0.39       888\n",
      "\n",
      "    accuracy                           0.64      4470\n",
      "   macro avg       0.58      0.62      0.57      4470\n",
      "weighted avg       0.75      0.64      0.67      4470\n",
      "\n",
      "\n",
      "RF Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.66      0.73      3582\n",
      "           1       0.26      0.48      0.33       888\n",
      "\n",
      "    accuracy                           0.62      4470\n",
      "   macro avg       0.54      0.57      0.53      4470\n",
      "weighted avg       0.72      0.62      0.65      4470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comparing all three models\n",
    "print('LogReg Validation set Accuracy:  {:.4}%'.format(log_reg.score(X_test, y_test) * 100))\n",
    "print('XGBoost Validation set Accuracy: {:.4}%'.format(XGB.score(X_test, y_test) * 100))\n",
    "print('Forest Validation set Accuracy:  {:.4}%'.format(forest.score(X_test, y_test) * 100))\n",
    "\n",
    "log_pred=log_reg.predict(X_test)\n",
    "XGB_pred=XGB.predict(X_test)\n",
    "RF_pred=forest.predict(X_test)\n",
    "\n",
    "print('\\nLogistic Regression Report:')\n",
    "print(classification_report(y_test, log_pred))\n",
    "print('\\nXGBoost Report:')\n",
    "print(classification_report(y_test, XGB_pred))\n",
    "print('\\nRF Report:')\n",
    "print(classification_report(y_test, RF_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5489b689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Models with Default Parameters:\n",
      "\n",
      "Logistic Regression F1:\n",
      "0.4150293002412961\n",
      "\n",
      "XGB F1:\n",
      "0.3933809702895825\n",
      "\n",
      "RF F1:\n",
      "0.3320220298977183\n"
     ]
    }
   ],
   "source": [
    "print('Baseline Models with Default Parameters:\\n')\n",
    "print('Logistic Regression F1:')\n",
    "print(f1_score(y_test, log_pred))\n",
    "print('\\nXGB F1:')\n",
    "print(f1_score(y_test, XGB_pred))\n",
    "print('\\nRF F1:')\n",
    "print(f1_score(y_test, RF_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f56a6b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2.218814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1.731091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.679886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.289511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.269563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-2.086772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-2.176576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-2.454484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-2.695218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-2.834656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        coef\n",
       "70  2.218814\n",
       "66  1.731091\n",
       "41  1.679886\n",
       "21  1.289511\n",
       "42  1.269563\n",
       "..       ...\n",
       "96 -2.086772\n",
       "33 -2.176576\n",
       "89 -2.454484\n",
       "50 -2.695218\n",
       "80 -2.834656\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef = log_reg.coef_[0]\n",
    "logreg_coef = pd.DataFrame(coef, \n",
    "             X_train_SMOTE.columns, \n",
    "             columns=['coef']).sort_values(by='coef', ascending=False)\n",
    "logreg_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55272442",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a4fdd6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = ['accuracy','f1']\n",
    "optimize = 'f1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a50ab86",
   "metadata": {},
   "source": [
    "### Logistic Regression Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "19fc21d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search found the following optimal parameters: \n",
      "C: 1\n",
      "class_weight: 'balanced'\n",
      "penalty: 'l2'\n",
      "solver: 'liblinear'\n"
     ]
    }
   ],
   "source": [
    "# Our hyperparameter grid, with some other options as comments\n",
    "logreg_params = {\n",
    "    \"C\" : [0.01, 0.1, 1],\n",
    "    'class_weight': ['balanced'],\n",
    "    \"penalty\" : [\"l1\",\"l2\"],\n",
    "    'solver':['liblinear']\n",
    "}\n",
    "\n",
    "# Searching Parameters\n",
    "rand_logreg = RandomizedSearchCV(log_reg, logreg_params, scoring=measure, refit=optimize, cv=3, n_jobs=1, n_iter=6, random_state=42)\n",
    "rand_logreg.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "\n",
    "logreg_best_params = rand_logreg.best_params_\n",
    "\n",
    "print('Grid Search found the following optimal parameters: ')\n",
    "for param_name in sorted(logreg_best_params.keys()):\n",
    "    print('%s: %r' % (param_name, logreg_best_params[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "40eec842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Logistic Regression F1: 0.4150293002412961\n",
      "Tuned Logistic Regression F1: 0.4150293002412961\n"
     ]
    }
   ],
   "source": [
    "# Comparison to Baseline\n",
    "Rlogreg_pred=rand_logreg.predict(X_test)\n",
    "print('Baseline Logistic Regression F1: ' + str(f1_score(y_test, log_pred)))\n",
    "print('Tuned Logistic Regression F1: ' + str(f1_score(y_test, Rlogreg_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7d54ca",
   "metadata": {},
   "source": [
    "## XGBoost Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b4d770aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:00:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:00:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:00:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:00:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:00:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:00:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:00:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:00:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:00:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:00:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:00:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"grow_policy\", \"interaction_constraints\", \"max_bin\", \"max_cat_to_onehot\", \"max_delta_step\", \"max_depth\", \"max_leaves\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"sampling_method\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search found the following optimal parameters: \n",
      "alpha: 0.01\n",
      "booster: 'gbtree'\n",
      "eval_metric: 'mlogloss'\n",
      "lambda: 1\n",
      "learning_rate: 0.25\n",
      "max_depth: 7\n",
      "min_child_weight: 1\n",
      "n_estimators: 120\n"
     ]
    }
   ],
   "source": [
    "# Our hyperparameter grid, with some other options as comments\n",
    "\n",
    "XGB_params = {\n",
    "    'learning_rate' : [0.2, 0.25, 0.3, 0.35],\n",
    "    'min_child_weight' : [1, 3, 5, 7],\n",
    "#     'gamma' : [0.0, 0.1, 0.2 , 0.3, 0.4],\n",
    "#     'colsample_bytree' : [0.3, 0.4, 0.5 , 0.7],\n",
    "    'alpha' : [0.01], # L1 regularization\n",
    "#     'eta': [1, 0.1, 0.01,],\n",
    "    'eval_metric':['mlogloss'],\n",
    "    'lambda':[1], # L2 regularization\n",
    "    'booster' : ['gbtree', 'gblinear', 'dart'],\n",
    "    'max_depth': [5, 7],\n",
    "    'n_estimators': [30, 60, 120],\n",
    "#     'scale_pos_weight' : [.9, 1, 1.1] # apparently deals with imbalanced data - not needed since training data is balanced \n",
    "#     'subsample': [0.5]\n",
    "}\n",
    "\n",
    "# Searching Parameters\n",
    "rand_XGB = GridSearchCV(XGB, XGB_params, scoring=measure, refit=optimize, cv=3, n_jobs=1)\n",
    "rand_XGB.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "\n",
    "XGB_best_parameters = rand_XGB.best_params_\n",
    "\n",
    "print('Grid Search found the following optimal parameters: ')\n",
    "for param_name in sorted(XGB_best_parameters.keys()):\n",
    "    print('%s: %r' % (param_name, XGB_best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "862819e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline XGBoost F1: 0.3933809702895825\n",
      "Tuned XGB F1: 0.3802505526897568\n"
     ]
    }
   ],
   "source": [
    "# Comparison to Baseline\n",
    "RXGB_pred=rand_XGB.predict(X_test)\n",
    "print('Baseline XGBoost F1: ' + str(f1_score(y_test, XGB_pred)))\n",
    "print('Tuned XGB F1: ' + str(f1_score(y_test, RXGB_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a656a2fd",
   "metadata": {},
   "source": [
    "## Random Forest Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "02aa1353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8D\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:306: UserWarning: The total space of parameters 4 is smaller than n_iter=5. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "C:\\Users\\8D\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\8D\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\8D\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\8D\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\8D\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\8D\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\8D\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\8D\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\8D\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search found the following optimal parameters: \n",
      "bootstrap: False\n",
      "class_weight: 'balanced'\n",
      "criterion: 'entropy'\n",
      "max_depth: 60\n",
      "max_features: 'auto'\n",
      "n_estimators: 100\n"
     ]
    }
   ],
   "source": [
    "# Our hyperparameter grid, with some other options as comments\n",
    "rf_params = {\n",
    "    'bootstrap' : [False],\n",
    "    'class_weight' : ['balanced'], # 'balanced_subsample',\n",
    "    'criterion' : ['entropy'], # 'gini', \n",
    "    'max_features' : ['auto'],\n",
    "    'max_depth' : [50, 60],\n",
    "    'n_estimators' : [100, 120]\n",
    "}\n",
    "\n",
    "# Searching parameters\n",
    "rand_rf = RandomizedSearchCV(forest, rf_params, scoring=measure, refit=optimize, cv=2, n_jobs=1, n_iter=5, random_state=42)\n",
    "rand_rf.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "\n",
    "rf_best_params = rand_rf.best_params_\n",
    "\n",
    "print('Grid Search found the following optimal parameters: ')\n",
    "for param_name in sorted(rf_best_params.keys()):\n",
    "    print('%s: %r' % (param_name, rf_best_params[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f647f2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Random Forest F1: 0.3320220298977183\n",
      "Tuned Random Forest F1: 0.3368779242875372\n"
     ]
    }
   ],
   "source": [
    "# Comparison to Baseline\n",
    "RRF_pred=rand_rf.predict(X_test)\n",
    "print('Baseline Random Forest F1: ' + str(f1_score(y_test, RF_pred)))\n",
    "print('Tuned Random Forest F1: ' + str(f1_score(y_test, RRF_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72553b7b",
   "metadata": {},
   "source": [
    "# Tuned Model Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3add114c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuned Logistic Regression Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.61      0.72      3582\n",
      "           1       0.30      0.68      0.42       888\n",
      "\n",
      "    accuracy                           0.62      4470\n",
      "   macro avg       0.59      0.64      0.57      4470\n",
      "weighted avg       0.77      0.62      0.66      4470\n",
      "\n",
      "\n",
      "Tuned XGBoost Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.65      0.74      3582\n",
      "           1       0.30      0.60      0.40       888\n",
      "\n",
      "    accuracy                           0.64      4470\n",
      "   macro avg       0.58      0.62      0.57      4470\n",
      "weighted avg       0.75      0.64      0.67      4470\n",
      "\n",
      "\n",
      "Tuned RF Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.70      0.76      3582\n",
      "           1       0.27      0.45      0.34       888\n",
      "\n",
      "    accuracy                           0.65      4470\n",
      "   macro avg       0.55      0.57      0.55      4470\n",
      "weighted avg       0.72      0.65      0.68      4470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nTuned Logistic Regression Report:')\n",
    "print(classification_report(y_test, Rlogreg_pred))\n",
    "print('\\nTuned XGBoost Report:')\n",
    "print(classification_report(y_test, RXGB_pred))\n",
    "print('\\nTuned RF Report:')\n",
    "print(classification_report(y_test, RRF_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "70476b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Accuracy Scores compared to Baseline:\n",
      "\n",
      "LogReg Validation Accuracy:         62.04%\n",
      "Tuned LogReg Validation Accuracy:   65.23%\n",
      "\n",
      "XGBoost Validation Accuracy:        63.91%\n",
      "Tuned XGBoost Validation Accuracy:  67.86%\n",
      "\n",
      "Forest Validation Accuracy:         62.01%\n",
      "Tuned Forest Validation Accuracy:   64.57%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Scores\n",
    "print('Tuned Accuracy Scores compared to Baseline:\\n')\n",
    "print('LogReg Validation Accuracy:         {:.4}%'.format(log_reg.score(X_test, y_test) * 100))\n",
    "print('Tuned LogReg Validation Accuracy:   {:.4}%'.format(rand_logreg.best_score_*100))\n",
    "print()\n",
    "print('XGBoost Validation Accuracy:        {:.4}%'.format(XGB.score(X_test, y_test) * 100))\n",
    "print('Tuned XGBoost Validation Accuracy:  {:.4}%'.format(rand_XGB.best_score_*100))\n",
    "print()\n",
    "print('Forest Validation Accuracy:         {:.4}%'.format(forest.score(X_test, y_test) * 100))\n",
    "print('Tuned Forest Validation Accuracy:   {:.4}%'.format(rand_rf.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "459ab9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logreg AUC: 64.2% -- False Positive Rate: 39.39% -- True Positive Rate: 67.79%\n",
      "XGB AUC:    62.28% -- False Positive Rate: 35.23% -- True Positive Rate: 59.8%\n",
      "RF AUC:     57.4% -- False Positive Rate: 29.79% -- True Positive Rate: 44.59%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ROC Curves for Tuned Sets\n",
    "logreg_fp, logreg_tp, thresholds=roc_curve(y_test, Rlogreg_pred, pos_label=1)\n",
    "logreg_AUC=auc(logreg_fp, logreg_tp)*100\n",
    "print(f'Logreg AUC: {round(logreg_AUC, 2)}% -- False Positive Rate: {round(logreg_fp[1] * 100, 2)}% -- True Positive Rate: {round(logreg_tp[1] * 100, 2)}%')\n",
    "plt.plot(logreg_fp, logreg_tp, color='blue',label = 'logreg_AUC = %0.2f' % logreg_AUC)\n",
    "\n",
    "XGB_fp, XGB_tp, thresholds=roc_curve(y_test, RXGB_pred, pos_label=1)\n",
    "XGB_AUC=auc(XGB_fp, XGB_tp)*100\n",
    "print(f'XGB AUC:    {round(XGB_AUC, 2)}% -- False Positive Rate: {round(XGB_fp[1] * 100, 2)}% -- True Positive Rate: {round(XGB_tp[1] * 100, 2)}%')\n",
    "plt.plot(XGB_fp, XGB_tp, color='red',label = 'XGB_AUC = %0.2f' % XGB_AUC)\n",
    "\n",
    "RF_fp, RF_tp, thresholds=roc_curve(y_test, RRF_pred, pos_label=1)\n",
    "RF_AUC=auc(RF_fp, RF_tp)*100\n",
    "print(f'RF AUC:     {round(RF_AUC, 2)}% -- False Positive Rate: {round(RF_fp[1] * 100, 2)}% -- True Positive Rate: {round(RF_tp[1] * 100, 2)}%')\n",
    "plt.plot(RF_fp, RF_tp, color='orange',label = 'RF_AUC = %0.2f' % RF_AUC)\n",
    "\n",
    "plt.title('ROC Curves')\n",
    "#plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % AUC)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6ccd7f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tuned Confusion Matrices\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(confusion_matrix(y_test, Rlogreg_pred), annot=True, fmt='g', ax=ax)\n",
    "ax.set_xlabel('Predicted labels'); ax.set_ylabel('True labels'); ax.set_title('LogReg Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(['Nonviral', 'Viral']); ax.yaxis.set_ticklabels(['Nonviral', 'Viral']);\n",
    "plt.show(ax)\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(confusion_matrix(y_test, RXGB_pred), annot=True, fmt='g', ax=ax)\n",
    "ax.set_xlabel('Predicted labels'); ax.set_ylabel('True labels'); ax.set_title('XGB Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(['Nonviral', 'Viral']); ax.yaxis.set_ticklabels(['Nonviral', 'Viral']);\n",
    "plt.show(ax)\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(confusion_matrix(y_test, RRF_pred), annot=True, fmt='g', ax=ax)\n",
    "ax.set_xlabel('Predicted labels'); ax.set_ylabel('True labels'); ax.set_title('Random Forest Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(['Nonviral', 'Viral']); ax.yaxis.set_ticklabels(['Nonviral', 'Viral']);\n",
    "plt.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "04410638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuned Logistic Regression Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.61      0.72      3582\n",
      "           1       0.30      0.68      0.42       888\n",
      "\n",
      "    accuracy                           0.62      4470\n",
      "   macro avg       0.59      0.64      0.57      4470\n",
      "weighted avg       0.77      0.62      0.66      4470\n",
      "\n",
      "\n",
      "Tuned XGBoost Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.65      0.74      3582\n",
      "           1       0.30      0.60      0.40       888\n",
      "\n",
      "    accuracy                           0.64      4470\n",
      "   macro avg       0.58      0.62      0.57      4470\n",
      "weighted avg       0.75      0.64      0.67      4470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nTuned Logistic Regression Report:')\n",
    "print(classification_report(y_test, Rlogreg_pred))\n",
    "print('\\nTuned XGBoost Report:')\n",
    "print(classification_report(y_test, RXGB_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c062ac06",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_8064/644020699.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\8D\\AppData\\Local\\Temp/ipykernel_8064/644020699.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Rlogreg_pred.\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Rlogreg_pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1c0908bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>followers</th>\n",
       "      <th>lists</th>\n",
       "      <th>confidence</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>0.400962</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>279</td>\n",
       "      <td>7</td>\n",
       "      <td>0.416909</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1489</td>\n",
       "      <td>54</td>\n",
       "      <td>0.238860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3487</td>\n",
       "      <td>84</td>\n",
       "      <td>0.624367</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>991</td>\n",
       "      <td>45</td>\n",
       "      <td>0.379968</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21487</th>\n",
       "      <td>7500</td>\n",
       "      <td>409</td>\n",
       "      <td>0.410173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21488</th>\n",
       "      <td>1809</td>\n",
       "      <td>32</td>\n",
       "      <td>0.405538</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21489</th>\n",
       "      <td>8621</td>\n",
       "      <td>137</td>\n",
       "      <td>0.456821</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21490</th>\n",
       "      <td>140062</td>\n",
       "      <td>3305</td>\n",
       "      <td>0.710264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21491</th>\n",
       "      <td>13715</td>\n",
       "      <td>143</td>\n",
       "      <td>0.451792</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21492 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       followers  lists  confidence  0  1  2  3  4  5  6  ...  89  90  91  92  \\\n",
       "0            200      2    0.400962  1  0  0  0  0  0  0  ...   0   0   0   0   \n",
       "1            279      7    0.416909  0  0  1  0  0  0  0  ...   0   0   0   0   \n",
       "2           1489     54    0.238860  0  0  0  0  0  0  0  ...   0   0   0   0   \n",
       "3           3487     84    0.624367  0  0  0  0  0  0  0  ...   0   0   0   0   \n",
       "4            991     45    0.379968  0  0  0  0  0  0  0  ...   0   0   0   0   \n",
       "...          ...    ...         ... .. .. .. .. .. .. ..  ...  ..  ..  ..  ..   \n",
       "21487       7500    409    0.410173  0  0  0  0  0  0  0  ...   0   0   0   0   \n",
       "21488       1809     32    0.405538  1  0  0  0  0  0  0  ...   0   0   0   0   \n",
       "21489       8621    137    0.456821  0  0  0  0  0  0  0  ...   0   0   0   0   \n",
       "21490     140062   3305    0.710264  0  0  0  0  0  0  0  ...   0   0   0   0   \n",
       "21491      13715    143    0.451792  0  0  0  0  0  0  0  ...   0   0   0   0   \n",
       "\n",
       "       93  94  95  96  97  98  \n",
       "0       0   0   0   0   0   0  \n",
       "1       0   0   0   0   0   0  \n",
       "2       0   0   0   0   0   0  \n",
       "3       0   0   0   0   0   0  \n",
       "4       0   0   0   0   0   0  \n",
       "...    ..  ..  ..  ..  ..  ..  \n",
       "21487   0   0   0   0   0   0  \n",
       "21488   0   0   0   0   0   0  \n",
       "21489   0   0   0   0   0   0  \n",
       "21490   0   0   0   0   0   0  \n",
       "21491   0   0   1   0   0   0  \n",
       "\n",
       "[21492 rows x 102 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b1a19f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>followers</th>\n",
       "      <th>lists</th>\n",
       "      <th>confidence</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6698</th>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>0.400962</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3643</th>\n",
       "      <td>279</td>\n",
       "      <td>7</td>\n",
       "      <td>0.416909</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6182</th>\n",
       "      <td>1489</td>\n",
       "      <td>54</td>\n",
       "      <td>0.238860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>3487</td>\n",
       "      <td>84</td>\n",
       "      <td>0.624367</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>991</td>\n",
       "      <td>45</td>\n",
       "      <td>0.379968</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>553</td>\n",
       "      <td>4</td>\n",
       "      <td>0.407077</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>1809</td>\n",
       "      <td>32</td>\n",
       "      <td>0.679152</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13668</th>\n",
       "      <td>3363</td>\n",
       "      <td>22</td>\n",
       "      <td>0.319774</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4314</th>\n",
       "      <td>900</td>\n",
       "      <td>7</td>\n",
       "      <td>0.257913</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14455</th>\n",
       "      <td>9359</td>\n",
       "      <td>147</td>\n",
       "      <td>0.357025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13409 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       followers  lists  confidence  0  1  2  3  4  5  6  ...  89  90  91  92  \\\n",
       "6698         200      2    0.400962  1  0  0  0  0  0  0  ...   0   0   0   0   \n",
       "3643         279      7    0.416909  0  0  1  0  0  0  0  ...   0   0   0   0   \n",
       "6182        1489     54    0.238860  0  0  0  0  0  0  0  ...   0   0   0   0   \n",
       "2655        3487     84    0.624367  0  0  0  0  0  0  0  ...   0   0   0   0   \n",
       "4597         991     45    0.379968  0  0  0  0  0  0  0  ...   0   0   0   0   \n",
       "...          ...    ...         ... .. .. .. .. .. .. ..  ...  ..  ..  ..  ..   \n",
       "1695         553      4    0.407077  0  0  0  0  0  1  0  ...   0   0   0   0   \n",
       "9090        1809     32    0.679152  0  0  0  0  0  0  0  ...   0   0   0   0   \n",
       "13668       3363     22    0.319774  0  0  0  0  0  0  0  ...   0   0   0   0   \n",
       "4314         900      7    0.257913  0  0  0  0  0  0  0  ...   0   0   0   0   \n",
       "14455       9359    147    0.357025  1  0  0  0  0  0  0  ...   0   0   0   0   \n",
       "\n",
       "       93  94  95  96  97  98  \n",
       "6698    0   0   0   0   0   0  \n",
       "3643    0   0   0   0   0   0  \n",
       "6182    0   0   0   0   0   0  \n",
       "2655    0   0   0   0   0   0  \n",
       "4597    0   0   0   0   0   0  \n",
       "...    ..  ..  ..  ..  ..  ..  \n",
       "1695    0   0   0   0   0   0  \n",
       "9090    0   0   0   0   0   0  \n",
       "13668   0   0   0   0   0   0  \n",
       "4314    0   0   0   0   0   0  \n",
       "14455   0   0   0   0   0   0  \n",
       "\n",
       "[13409 rows x 102 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e3b39030",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8D\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>viral</td>      <th>  R-squared:         </th> <td>   0.090</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.083</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   13.31</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 25 Sep 2022</td> <th>  Prob (F-statistic):</th> <td>1.23e-200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:14:26</td>     <th>  Log-Likelihood:    </th> <td> -6071.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 13409</td>      <th>  AIC:               </th> <td>1.234e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 13309</td>      <th>  BIC:               </th> <td>1.309e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    99</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td>    0.1226</td> <td>    0.013</td> <td>    9.648</td> <td> 0.000</td> <td>    0.098</td> <td>    0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>confidence</th> <td>    0.1656</td> <td>    0.029</td> <td>    5.650</td> <td> 0.000</td> <td>    0.108</td> <td>    0.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0</th>          <td>    0.0928</td> <td>    0.012</td> <td>    7.633</td> <td> 0.000</td> <td>    0.069</td> <td>    0.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1</th>          <td>   -0.1226</td> <td>    0.017</td> <td>   -7.340</td> <td> 0.000</td> <td>   -0.155</td> <td>   -0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2</th>          <td>    0.0342</td> <td>    0.019</td> <td>    1.774</td> <td> 0.076</td> <td>   -0.004</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3</th>          <td>   -0.0537</td> <td>    0.020</td> <td>   -2.632</td> <td> 0.009</td> <td>   -0.094</td> <td>   -0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th>          <td>   -0.0428</td> <td>    0.020</td> <td>   -2.115</td> <td> 0.034</td> <td>   -0.082</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5</th>          <td>    0.0586</td> <td>    0.021</td> <td>    2.780</td> <td> 0.005</td> <td>    0.017</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6</th>          <td>   -0.0529</td> <td>    0.022</td> <td>   -2.419</td> <td> 0.016</td> <td>   -0.096</td> <td>   -0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7</th>          <td>    0.1913</td> <td>    0.022</td> <td>    8.790</td> <td> 0.000</td> <td>    0.149</td> <td>    0.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8</th>          <td>    0.0474</td> <td>    0.025</td> <td>    1.891</td> <td> 0.059</td> <td>   -0.002</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9</th>          <td>   -0.1226</td> <td>    0.025</td> <td>   -4.979</td> <td> 0.000</td> <td>   -0.171</td> <td>   -0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10</th>         <td>    0.1007</td> <td>    0.025</td> <td>    4.038</td> <td> 0.000</td> <td>    0.052</td> <td>    0.150</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11</th>         <td>   -0.0492</td> <td>    0.025</td> <td>   -1.951</td> <td> 0.051</td> <td>   -0.099</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12</th>         <td>   -0.1262</td> <td>    0.025</td> <td>   -5.058</td> <td> 0.000</td> <td>   -0.175</td> <td>   -0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13</th>         <td>    0.1633</td> <td>    0.026</td> <td>    6.376</td> <td> 0.000</td> <td>    0.113</td> <td>    0.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14</th>         <td>   -0.1102</td> <td>    0.025</td> <td>   -4.369</td> <td> 0.000</td> <td>   -0.160</td> <td>   -0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15</th>         <td>   -0.0374</td> <td>    0.026</td> <td>   -1.444</td> <td> 0.149</td> <td>   -0.088</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16</th>         <td>   -0.0731</td> <td>    0.026</td> <td>   -2.784</td> <td> 0.005</td> <td>   -0.125</td> <td>   -0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>17</th>         <td>   -0.0601</td> <td>    0.025</td> <td>   -2.366</td> <td> 0.018</td> <td>   -0.110</td> <td>   -0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18</th>         <td>    0.1641</td> <td>    0.025</td> <td>    6.486</td> <td> 0.000</td> <td>    0.114</td> <td>    0.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>19</th>         <td>   -0.0651</td> <td>    0.027</td> <td>   -2.391</td> <td> 0.017</td> <td>   -0.118</td> <td>   -0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>20</th>         <td>   -0.0668</td> <td>    0.027</td> <td>   -2.497</td> <td> 0.013</td> <td>   -0.119</td> <td>   -0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21</th>         <td>    0.2584</td> <td>    0.027</td> <td>    9.656</td> <td> 0.000</td> <td>    0.206</td> <td>    0.311</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22</th>         <td>   -0.0957</td> <td>    0.028</td> <td>   -3.423</td> <td> 0.001</td> <td>   -0.150</td> <td>   -0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23</th>         <td>   -0.0982</td> <td>    0.030</td> <td>   -3.300</td> <td> 0.001</td> <td>   -0.157</td> <td>   -0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24</th>         <td>    0.2065</td> <td>    0.029</td> <td>    7.204</td> <td> 0.000</td> <td>    0.150</td> <td>    0.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25</th>         <td>    0.0476</td> <td>    0.029</td> <td>    1.654</td> <td> 0.098</td> <td>   -0.009</td> <td>    0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>26</th>         <td>    0.0229</td> <td>    0.030</td> <td>    0.771</td> <td> 0.441</td> <td>   -0.035</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>27</th>         <td>   -0.0858</td> <td>    0.031</td> <td>   -2.789</td> <td> 0.005</td> <td>   -0.146</td> <td>   -0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28</th>         <td>   -0.0767</td> <td>    0.031</td> <td>   -2.453</td> <td> 0.014</td> <td>   -0.138</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>29</th>         <td>   -0.0728</td> <td>    0.031</td> <td>   -2.325</td> <td> 0.020</td> <td>   -0.134</td> <td>   -0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>30</th>         <td>    0.0481</td> <td>    0.032</td> <td>    1.492</td> <td> 0.136</td> <td>   -0.015</td> <td>    0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>31</th>         <td>   -0.0592</td> <td>    0.032</td> <td>   -1.880</td> <td> 0.060</td> <td>   -0.121</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>32</th>         <td>   -0.0017</td> <td>    0.031</td> <td>   -0.054</td> <td> 0.957</td> <td>   -0.062</td> <td>    0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>33</th>         <td>   -0.1540</td> <td>    0.032</td> <td>   -4.825</td> <td> 0.000</td> <td>   -0.216</td> <td>   -0.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>34</th>         <td>    0.0155</td> <td>    0.032</td> <td>    0.482</td> <td> 0.630</td> <td>   -0.048</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>35</th>         <td>    0.0292</td> <td>    0.033</td> <td>    0.885</td> <td> 0.376</td> <td>   -0.035</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>36</th>         <td>   -0.0461</td> <td>    0.033</td> <td>   -1.399</td> <td> 0.162</td> <td>   -0.111</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>37</th>         <td>    0.0900</td> <td>    0.035</td> <td>    2.599</td> <td> 0.009</td> <td>    0.022</td> <td>    0.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>38</th>         <td>    0.0917</td> <td>    0.035</td> <td>    2.638</td> <td> 0.008</td> <td>    0.024</td> <td>    0.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>39</th>         <td>    0.1877</td> <td>    0.034</td> <td>    5.550</td> <td> 0.000</td> <td>    0.121</td> <td>    0.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>40</th>         <td>    0.2070</td> <td>    0.037</td> <td>    5.598</td> <td> 0.000</td> <td>    0.134</td> <td>    0.279</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>41</th>         <td>    0.3765</td> <td>    0.038</td> <td>   10.036</td> <td> 0.000</td> <td>    0.303</td> <td>    0.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>42</th>         <td>    0.2430</td> <td>    0.039</td> <td>    6.258</td> <td> 0.000</td> <td>    0.167</td> <td>    0.319</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>43</th>         <td>    0.0197</td> <td>    0.040</td> <td>    0.497</td> <td> 0.619</td> <td>   -0.058</td> <td>    0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>44</th>         <td>    0.0959</td> <td>    0.039</td> <td>    2.484</td> <td> 0.013</td> <td>    0.020</td> <td>    0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>45</th>         <td>    0.0152</td> <td>    0.039</td> <td>    0.387</td> <td> 0.698</td> <td>   -0.062</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>46</th>         <td>   -0.0733</td> <td>    0.040</td> <td>   -1.827</td> <td> 0.068</td> <td>   -0.152</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>47</th>         <td>   -0.0951</td> <td>    0.039</td> <td>   -2.450</td> <td> 0.014</td> <td>   -0.171</td> <td>   -0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>48</th>         <td>   -0.0580</td> <td>    0.040</td> <td>   -1.452</td> <td> 0.147</td> <td>   -0.136</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>49</th>         <td>   -0.0686</td> <td>    0.041</td> <td>   -1.688</td> <td> 0.091</td> <td>   -0.148</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>50</th>         <td>   -0.1649</td> <td>    0.041</td> <td>   -4.021</td> <td> 0.000</td> <td>   -0.245</td> <td>   -0.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>51</th>         <td>    0.0114</td> <td>    0.040</td> <td>    0.285</td> <td> 0.776</td> <td>   -0.067</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>52</th>         <td>    0.0541</td> <td>    0.043</td> <td>    1.257</td> <td> 0.209</td> <td>   -0.030</td> <td>    0.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>53</th>         <td>   -0.0879</td> <td>    0.045</td> <td>   -1.950</td> <td> 0.051</td> <td>   -0.176</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>54</th>         <td>   -0.0054</td> <td>    0.043</td> <td>   -0.124</td> <td> 0.902</td> <td>   -0.090</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>55</th>         <td>   -0.0602</td> <td>    0.042</td> <td>   -1.430</td> <td> 0.153</td> <td>   -0.143</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>56</th>         <td>   -0.0705</td> <td>    0.043</td> <td>   -1.649</td> <td> 0.099</td> <td>   -0.154</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>57</th>         <td>   -0.0918</td> <td>    0.042</td> <td>   -2.187</td> <td> 0.029</td> <td>   -0.174</td> <td>   -0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>58</th>         <td>   -0.0647</td> <td>    0.043</td> <td>   -1.504</td> <td> 0.133</td> <td>   -0.149</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>59</th>         <td>    0.0865</td> <td>    0.046</td> <td>    1.893</td> <td> 0.058</td> <td>   -0.003</td> <td>    0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>60</th>         <td>   -0.1201</td> <td>    0.045</td> <td>   -2.682</td> <td> 0.007</td> <td>   -0.208</td> <td>   -0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61</th>         <td>    0.0249</td> <td>    0.044</td> <td>    0.568</td> <td> 0.570</td> <td>   -0.061</td> <td>    0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62</th>         <td>   -0.0632</td> <td>    0.048</td> <td>   -1.311</td> <td> 0.190</td> <td>   -0.158</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>63</th>         <td>   -0.0425</td> <td>    0.047</td> <td>   -0.896</td> <td> 0.370</td> <td>   -0.136</td> <td>    0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>64</th>         <td>   -0.0215</td> <td>    0.047</td> <td>   -0.460</td> <td> 0.645</td> <td>   -0.113</td> <td>    0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>65</th>         <td>   -0.0378</td> <td>    0.046</td> <td>   -0.820</td> <td> 0.412</td> <td>   -0.128</td> <td>    0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>66</th>         <td>    0.3977</td> <td>    0.045</td> <td>    8.762</td> <td> 0.000</td> <td>    0.309</td> <td>    0.487</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>67</th>         <td>   -0.0001</td> <td>    0.047</td> <td>   -0.002</td> <td> 0.998</td> <td>   -0.092</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>68</th>         <td>   -0.0880</td> <td>    0.044</td> <td>   -2.006</td> <td> 0.045</td> <td>   -0.174</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>69</th>         <td>   -0.0841</td> <td>    0.046</td> <td>   -1.839</td> <td> 0.066</td> <td>   -0.174</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>70</th>         <td>    0.4947</td> <td>    0.046</td> <td>   10.738</td> <td> 0.000</td> <td>    0.404</td> <td>    0.585</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>71</th>         <td>    0.0360</td> <td>    0.048</td> <td>    0.752</td> <td> 0.452</td> <td>   -0.058</td> <td>    0.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>72</th>         <td>    0.2669</td> <td>    0.049</td> <td>    5.444</td> <td> 0.000</td> <td>    0.171</td> <td>    0.363</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>73</th>         <td>   -0.0935</td> <td>    0.049</td> <td>   -1.908</td> <td> 0.056</td> <td>   -0.190</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>74</th>         <td>    0.0292</td> <td>    0.048</td> <td>    0.610</td> <td> 0.542</td> <td>   -0.065</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>75</th>         <td>   -0.1255</td> <td>    0.049</td> <td>   -2.540</td> <td> 0.011</td> <td>   -0.222</td> <td>   -0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>76</th>         <td>   -0.0194</td> <td>    0.052</td> <td>   -0.376</td> <td> 0.707</td> <td>   -0.121</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>77</th>         <td>   -0.1131</td> <td>    0.048</td> <td>   -2.345</td> <td> 0.019</td> <td>   -0.208</td> <td>   -0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>78</th>         <td>    0.0405</td> <td>    0.052</td> <td>    0.776</td> <td> 0.438</td> <td>   -0.062</td> <td>    0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>79</th>         <td>    0.0689</td> <td>    0.054</td> <td>    1.284</td> <td> 0.199</td> <td>   -0.036</td> <td>    0.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>80</th>         <td>   -0.2243</td> <td>    0.053</td> <td>   -4.196</td> <td> 0.000</td> <td>   -0.329</td> <td>   -0.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>81</th>         <td>   -0.0749</td> <td>    0.057</td> <td>   -1.325</td> <td> 0.185</td> <td>   -0.186</td> <td>    0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>82</th>         <td>    0.0196</td> <td>    0.052</td> <td>    0.377</td> <td> 0.706</td> <td>   -0.083</td> <td>    0.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>83</th>         <td>   -0.0374</td> <td>    0.052</td> <td>   -0.718</td> <td> 0.473</td> <td>   -0.140</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>84</th>         <td>    0.2178</td> <td>    0.052</td> <td>    4.218</td> <td> 0.000</td> <td>    0.117</td> <td>    0.319</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>85</th>         <td>   -0.0971</td> <td>    0.052</td> <td>   -1.876</td> <td> 0.061</td> <td>   -0.198</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>86</th>         <td>   -0.1040</td> <td>    0.055</td> <td>   -1.900</td> <td> 0.057</td> <td>   -0.211</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>87</th>         <td>    0.0918</td> <td>    0.054</td> <td>    1.694</td> <td> 0.090</td> <td>   -0.014</td> <td>    0.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>88</th>         <td>   -0.1104</td> <td>    0.054</td> <td>   -2.059</td> <td> 0.040</td> <td>   -0.216</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>89</th>         <td>   -0.1723</td> <td>    0.054</td> <td>   -3.211</td> <td> 0.001</td> <td>   -0.277</td> <td>   -0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>90</th>         <td>    0.0149</td> <td>    0.055</td> <td>    0.269</td> <td> 0.788</td> <td>   -0.094</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>91</th>         <td>   -0.0459</td> <td>    0.058</td> <td>   -0.793</td> <td> 0.428</td> <td>   -0.159</td> <td>    0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>92</th>         <td>    0.1410</td> <td>    0.061</td> <td>    2.322</td> <td> 0.020</td> <td>    0.022</td> <td>    0.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>93</th>         <td>   -0.1371</td> <td>    0.061</td> <td>   -2.259</td> <td> 0.024</td> <td>   -0.256</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>94</th>         <td>   -0.1347</td> <td>    0.073</td> <td>   -1.846</td> <td> 0.065</td> <td>   -0.278</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>95</th>         <td>   -0.0012</td> <td>    0.064</td> <td>   -0.018</td> <td> 0.986</td> <td>   -0.127</td> <td>    0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>96</th>         <td>   -0.1664</td> <td>    0.068</td> <td>   -2.445</td> <td> 0.015</td> <td>   -0.300</td> <td>   -0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>97</th>         <td>    0.0626</td> <td>    0.073</td> <td>    0.859</td> <td> 0.390</td> <td>   -0.080</td> <td>    0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98</th>         <td>   -0.1157</td> <td>    0.083</td> <td>   -1.398</td> <td> 0.162</td> <td>   -0.278</td> <td>    0.046</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>2492.428</td> <th>  Durbin-Watson:     </th> <td>   2.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>4152.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.349</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 3.396</td>  <th>  Cond. No.          </th> <td>1.28e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 9.65e-29. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  viral   R-squared:                       0.090\n",
       "Model:                            OLS   Adj. R-squared:                  0.083\n",
       "Method:                 Least Squares   F-statistic:                     13.31\n",
       "Date:                Sun, 25 Sep 2022   Prob (F-statistic):          1.23e-200\n",
       "Time:                        12:14:26   Log-Likelihood:                -6071.7\n",
       "No. Observations:               13409   AIC:                         1.234e+04\n",
       "Df Residuals:                   13309   BIC:                         1.309e+04\n",
       "Df Model:                          99                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.1226      0.013      9.648      0.000       0.098       0.148\n",
       "confidence     0.1656      0.029      5.650      0.000       0.108       0.223\n",
       "0              0.0928      0.012      7.633      0.000       0.069       0.117\n",
       "1             -0.1226      0.017     -7.340      0.000      -0.155      -0.090\n",
       "2              0.0342      0.019      1.774      0.076      -0.004       0.072\n",
       "3             -0.0537      0.020     -2.632      0.009      -0.094      -0.014\n",
       "4             -0.0428      0.020     -2.115      0.034      -0.082      -0.003\n",
       "5              0.0586      0.021      2.780      0.005       0.017       0.100\n",
       "6             -0.0529      0.022     -2.419      0.016      -0.096      -0.010\n",
       "7              0.1913      0.022      8.790      0.000       0.149       0.234\n",
       "8              0.0474      0.025      1.891      0.059      -0.002       0.096\n",
       "9             -0.1226      0.025     -4.979      0.000      -0.171      -0.074\n",
       "10             0.1007      0.025      4.038      0.000       0.052       0.150\n",
       "11            -0.0492      0.025     -1.951      0.051      -0.099       0.000\n",
       "12            -0.1262      0.025     -5.058      0.000      -0.175      -0.077\n",
       "13             0.1633      0.026      6.376      0.000       0.113       0.214\n",
       "14            -0.1102      0.025     -4.369      0.000      -0.160      -0.061\n",
       "15            -0.0374      0.026     -1.444      0.149      -0.088       0.013\n",
       "16            -0.0731      0.026     -2.784      0.005      -0.125      -0.022\n",
       "17            -0.0601      0.025     -2.366      0.018      -0.110      -0.010\n",
       "18             0.1641      0.025      6.486      0.000       0.114       0.214\n",
       "19            -0.0651      0.027     -2.391      0.017      -0.118      -0.012\n",
       "20            -0.0668      0.027     -2.497      0.013      -0.119      -0.014\n",
       "21             0.2584      0.027      9.656      0.000       0.206       0.311\n",
       "22            -0.0957      0.028     -3.423      0.001      -0.150      -0.041\n",
       "23            -0.0982      0.030     -3.300      0.001      -0.157      -0.040\n",
       "24             0.2065      0.029      7.204      0.000       0.150       0.263\n",
       "25             0.0476      0.029      1.654      0.098      -0.009       0.104\n",
       "26             0.0229      0.030      0.771      0.441      -0.035       0.081\n",
       "27            -0.0858      0.031     -2.789      0.005      -0.146      -0.025\n",
       "28            -0.0767      0.031     -2.453      0.014      -0.138      -0.015\n",
       "29            -0.0728      0.031     -2.325      0.020      -0.134      -0.011\n",
       "30             0.0481      0.032      1.492      0.136      -0.015       0.111\n",
       "31            -0.0592      0.032     -1.880      0.060      -0.121       0.003\n",
       "32            -0.0017      0.031     -0.054      0.957      -0.062       0.059\n",
       "33            -0.1540      0.032     -4.825      0.000      -0.216      -0.091\n",
       "34             0.0155      0.032      0.482      0.630      -0.048       0.078\n",
       "35             0.0292      0.033      0.885      0.376      -0.035       0.094\n",
       "36            -0.0461      0.033     -1.399      0.162      -0.111       0.018\n",
       "37             0.0900      0.035      2.599      0.009       0.022       0.158\n",
       "38             0.0917      0.035      2.638      0.008       0.024       0.160\n",
       "39             0.1877      0.034      5.550      0.000       0.121       0.254\n",
       "40             0.2070      0.037      5.598      0.000       0.134       0.279\n",
       "41             0.3765      0.038     10.036      0.000       0.303       0.450\n",
       "42             0.2430      0.039      6.258      0.000       0.167       0.319\n",
       "43             0.0197      0.040      0.497      0.619      -0.058       0.097\n",
       "44             0.0959      0.039      2.484      0.013       0.020       0.172\n",
       "45             0.0152      0.039      0.387      0.698      -0.062       0.092\n",
       "46            -0.0733      0.040     -1.827      0.068      -0.152       0.005\n",
       "47            -0.0951      0.039     -2.450      0.014      -0.171      -0.019\n",
       "48            -0.0580      0.040     -1.452      0.147      -0.136       0.020\n",
       "49            -0.0686      0.041     -1.688      0.091      -0.148       0.011\n",
       "50            -0.1649      0.041     -4.021      0.000      -0.245      -0.085\n",
       "51             0.0114      0.040      0.285      0.776      -0.067       0.090\n",
       "52             0.0541      0.043      1.257      0.209      -0.030       0.138\n",
       "53            -0.0879      0.045     -1.950      0.051      -0.176       0.000\n",
       "54            -0.0054      0.043     -0.124      0.902      -0.090       0.080\n",
       "55            -0.0602      0.042     -1.430      0.153      -0.143       0.022\n",
       "56            -0.0705      0.043     -1.649      0.099      -0.154       0.013\n",
       "57            -0.0918      0.042     -2.187      0.029      -0.174      -0.010\n",
       "58            -0.0647      0.043     -1.504      0.133      -0.149       0.020\n",
       "59             0.0865      0.046      1.893      0.058      -0.003       0.176\n",
       "60            -0.1201      0.045     -2.682      0.007      -0.208      -0.032\n",
       "61             0.0249      0.044      0.568      0.570      -0.061       0.111\n",
       "62            -0.0632      0.048     -1.311      0.190      -0.158       0.031\n",
       "63            -0.0425      0.047     -0.896      0.370      -0.136       0.051\n",
       "64            -0.0215      0.047     -0.460      0.645      -0.113       0.070\n",
       "65            -0.0378      0.046     -0.820      0.412      -0.128       0.053\n",
       "66             0.3977      0.045      8.762      0.000       0.309       0.487\n",
       "67            -0.0001      0.047     -0.002      0.998      -0.092       0.092\n",
       "68            -0.0880      0.044     -2.006      0.045      -0.174      -0.002\n",
       "69            -0.0841      0.046     -1.839      0.066      -0.174       0.006\n",
       "70             0.4947      0.046     10.738      0.000       0.404       0.585\n",
       "71             0.0360      0.048      0.752      0.452      -0.058       0.130\n",
       "72             0.2669      0.049      5.444      0.000       0.171       0.363\n",
       "73            -0.0935      0.049     -1.908      0.056      -0.190       0.003\n",
       "74             0.0292      0.048      0.610      0.542      -0.065       0.123\n",
       "75            -0.1255      0.049     -2.540      0.011      -0.222      -0.029\n",
       "76            -0.0194      0.052     -0.376      0.707      -0.121       0.082\n",
       "77            -0.1131      0.048     -2.345      0.019      -0.208      -0.019\n",
       "78             0.0405      0.052      0.776      0.438      -0.062       0.143\n",
       "79             0.0689      0.054      1.284      0.199      -0.036       0.174\n",
       "80            -0.2243      0.053     -4.196      0.000      -0.329      -0.120\n",
       "81            -0.0749      0.057     -1.325      0.185      -0.186       0.036\n",
       "82             0.0196      0.052      0.377      0.706      -0.083       0.122\n",
       "83            -0.0374      0.052     -0.718      0.473      -0.140       0.065\n",
       "84             0.2178      0.052      4.218      0.000       0.117       0.319\n",
       "85            -0.0971      0.052     -1.876      0.061      -0.198       0.004\n",
       "86            -0.1040      0.055     -1.900      0.057      -0.211       0.003\n",
       "87             0.0918      0.054      1.694      0.090      -0.014       0.198\n",
       "88            -0.1104      0.054     -2.059      0.040      -0.216      -0.005\n",
       "89            -0.1723      0.054     -3.211      0.001      -0.277      -0.067\n",
       "90             0.0149      0.055      0.269      0.788      -0.094       0.123\n",
       "91            -0.0459      0.058     -0.793      0.428      -0.159       0.067\n",
       "92             0.1410      0.061      2.322      0.020       0.022       0.260\n",
       "93            -0.1371      0.061     -2.259      0.024      -0.256      -0.018\n",
       "94            -0.1347      0.073     -1.846      0.065      -0.278       0.008\n",
       "95            -0.0012      0.064     -0.018      0.986      -0.127       0.124\n",
       "96            -0.1664      0.068     -2.445      0.015      -0.300      -0.033\n",
       "97             0.0626      0.073      0.859      0.390      -0.080       0.206\n",
       "98            -0.1157      0.083     -1.398      0.162      -0.278       0.046\n",
       "==============================================================================\n",
       "Omnibus:                     2492.428   Durbin-Watson:                   2.012\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             4152.308\n",
       "Skew:                           1.349   Prob(JB):                         0.00\n",
       "Kurtosis:                       3.396   Cond. No.                     1.28e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 9.65e-29. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "### OLS multi-linear regression\n",
    "\n",
    "X_train_1 = sm.add_constant(X_train) # Adding constant\n",
    "model = sm.OLS(y_train, X_train_1) # defining model parameters\n",
    "results = model.fit() # Fitting model\n",
    "\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9452f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
