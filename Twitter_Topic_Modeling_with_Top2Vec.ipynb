{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4f233d4",
   "metadata": {},
   "source": [
    "# Topic Modeling with Top2Vec\n",
    "Here we'll build a NLP Pipeline to interpret our tweet text data. Then we can see how these topics might correlate with virality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60a05eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from top2vec import Top2Vec\n",
    "import regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d92f33b",
   "metadata": {},
   "source": [
    "### Loading and Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72be5733",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/combined_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f2e7641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document List Creation\n",
    "docs = data.tweet.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f93fb32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document List Cleaning\n",
    "docs = [d.replace(\"https\", \"\") for d in docs]\n",
    "docs = [d.replace(\"daysofcode\", \"\") for d in docs]\n",
    "docs = [d.replace(\".co\", \"\") for d in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8153fbb4",
   "metadata": {},
   "source": [
    "### Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ce0e98f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 13:12:36,371 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Occurrance Count: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 13:12:41,175 - top2vec - INFO - Downloading universal-sentence-encoder model\n",
      "INFO:top2vec:Downloading universal-sentence-encoder model\n",
      "2022-10-15 13:39:28,090 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2022-10-15 13:39:38,752 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2022-10-15 13:40:12,040 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2022-10-15 13:40:15,665 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n"
     ]
    }
   ],
   "source": [
    "# Model Parameters\n",
    "min_ct_for_topic = int(len(docs) / 700)\n",
    "print(f\"Min Occurrance Count: {min_ct_for_topic}\")\n",
    "\n",
    "# Model Training\n",
    "model = Top2Vec(docs, embedding_model='universal-sentence-encoder', min_count=min_ct_for_topic, workers=8, ngram_vocab=False, speed=\"deep-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db936a9c",
   "metadata": {},
   "source": [
    "### Interpreting Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3792d8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Topics:  228\n",
      "\n",
      "[1706 1700 1657 1310 1304 1183 1149 1084 1035  984  892  871  859  852\n",
      "  688  669  653  648  618  616  594  586  582  577  573  557  551  541\n",
      "  520  518  504  502  481  464  448  444  443  442  414  414  413  409\n",
      "  392  392  385  380  372  357  352  352  349  346  346  342  337  335\n",
      "  334  331  325  320  319  315  314  313  309  306  305  302  296  293\n",
      "  292  289  285  278  277  277  276  275  272  271  270  265  262  261\n",
      "  261  255  246  242  240  238  236  234  228  228  227  222  211  210\n",
      "  208  207  204  202  200  197  195  194  193  193  190  186  184  184\n",
      "  177  176  175  173  170  169  168  167  167  165  162  162  162  161\n",
      "  160  160  159  156  155  151  150  147  140  140  139  137  137  137\n",
      "  136  136  134  133  133  131  128  128  126  125  125  123  123  122\n",
      "  121  120  119  118  117  116  114  114  113  112  112  112  110  109\n",
      "  108  108  106  105  104  101  101  100   99   99   95   94   92   92\n",
      "   92   89   89   89   86   86   84   84   82   79   79   78   78   76\n",
      "   76   76   74   74   72   71   71   69   68   67   67   63   63   60\n",
      "   58   58   57   57   57   54   54   54   52   52   51   51   49   48\n",
      "   46   43   40   35]\n"
     ]
    }
   ],
   "source": [
    "# Peek topic length & distribution\n",
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print(\"Number of Topics: \",len(topic_nums))\n",
    "print()\n",
    "print(topic_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3251341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Examine all topics\n",
    "# for words, scores, num in zip(topic_words, word_scores, topic_nums):\n",
    "#     print(num)\n",
    "#     print(f\"Topic Keywords: {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1ab021de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract topic data\n",
    "topic_words, word_scores, topic_nums = model.get_topics()\n",
    "\n",
    "# Individual Tweet Lookup\n",
    "def tweet_topic_lookup(tweet_int, model, data):\n",
    "    display(data.iloc[[tweet_int]].style.set_properties(**{'text-align': 'left'})) # Testing document correlation with DF by looking up tweets\n",
    "    display(model.get_documents_topics([tweet_int])[1]) # Look at top2vec confidence of single tweet\n",
    "    display(model.get_documents_topics([tweet_int])[2]) # Look at top2vec topic keywords compared to a single tweet\n",
    "\n",
    "# Look up topic and get information and examples\n",
    "def examine_topic(topic, model):\n",
    "    print('Main Keywords:')\n",
    "    print(topic_words[topic])\n",
    "    print('----------')\n",
    "    print('Keyword Importance:')\n",
    "    print(word_scores[topic]) # Look at word scores by topic\n",
    "    print('----------')\n",
    "    print('Sample Tweets:')\n",
    "    print('----------')\n",
    "    documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=topic, num_docs=5)\n",
    "    for doc, score, doc_id in zip(documents, document_scores, document_ids):\n",
    "        print(f\"### Document: {doc_id}, Score: {score} ###\")\n",
    "        print(doc)\n",
    "        print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3404d54d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Keywords:\n",
      "['nlp' 'deepmind' 'neural' 'tensorflow' 'kaggle' 'generative' 'openai'\n",
      " 'computational' 'algorithms' 'ai' 'coursera' 'dataset' 'learning'\n",
      " 'freecodecamp' 'algorithm' 'bigdata' 'datasets' 'training' 'webinar'\n",
      " 'introducing' 'neuroscience' 'taught' 'data' 'mit' 'blockchain'\n",
      " 'cognitive' 'iot' 'reinforcement' 'recognition' 'stanford'\n",
      " 'classification' 'econometrics' 'pytorch' 'quantum' 'gpu' 'text'\n",
      " 'framework' 'knows' 'featuring' 'ml' 'developed' 'inference'\n",
      " 'interactive' 'api' 'computing' 'github' 'trained' 'abstract' 'improved'\n",
      " 'study']\n",
      "----------\n",
      "Keyword Importance:\n",
      "[0.39640322 0.35023597 0.33974016 0.32590184 0.322042   0.30702546\n",
      " 0.28158936 0.27387494 0.26199976 0.25702336 0.25467148 0.2471179\n",
      " 0.22766978 0.21247593 0.2123543  0.21055162 0.20482895 0.19830191\n",
      " 0.1968064  0.19650778 0.19054145 0.18923914 0.18612158 0.18451017\n",
      " 0.18440522 0.18187502 0.18078029 0.17971212 0.17963073 0.17637622\n",
      " 0.17589478 0.17390436 0.17373878 0.17256013 0.17111129 0.17054862\n",
      " 0.16924141 0.16785622 0.16488472 0.1641979  0.16289835 0.16223732\n",
      " 0.16081433 0.160229   0.15913987 0.15839005 0.15759833 0.15668224\n",
      " 0.15502283 0.15461028]\n",
      "----------\n",
      "Sample Tweets:\n",
      "----------\n",
      "### Document: 62221, Score: 0.7487335801124573 ###\n",
      "Cool work from @LightOnIO on applying @arild567's Direct Feedback Alignment for training neural nets on a variety of tasks (recommender systems, neural  view synthesis, graph CNNs, NLP): ://t/g2RXyhpPOr\n",
      "----------\n",
      "### Document: 14353, Score: 0.7273207306861877 ###\n",
      "Slides for MIT's Machine Learning for Healthcare '22 class w/@DrMadhurNayan here: ://t/xMeWlxYp3y! Featuring tons of new material including clinical NLP by @MonicaNAgrawal, imaging @YalaTweets, human-AI interaction @HsseinMzannar, fairness @irenetrampoline, &amp;dataset shift\n",
      "----------\n",
      "### Document: 25607, Score: 0.7224843502044678 ###\n",
      "In Fall 2021, CMU is updating its NLP curriculum, and 11-747 \"Neural Networks for NLP\" is being repurposed into 11-711 \"Advanced NLP\", the flagship research-based NLP class 😃\n",
      "More NLP fundamentals, still neural network methods. Stay tuned! (CMU students, please register!) ://t/xE08QoFWHY\n",
      "----------\n",
      "### Document: 25635, Score: 0.7206293940544128 ###\n",
      "2021 version of CMU \"Neural Networks for NLP\" slides (://t/X2rd0eHNiW) and videos (://t/6Yi2cZmOLE) are being posted in real time! Check it out for a comprehensive graduate-level class on NLP! New this year: assignment on implementing parts of your own NN toolkit. ://t/kpyA1nQAE5\n",
      "----------\n",
      "### Document: 17512, Score: 0.7164576053619385 ###\n",
      "I am looking for a postdoc to work on deep learning methods for computer vision!\n",
      "\n",
      "Particularly:\n",
      "- reinforcement learning\n",
      "- vision transformers\n",
      "- Bayesian neural networks\n",
      "- causal inference\n",
      "- transfer learning\n",
      "- self-supervised learning\n",
      "\n",
      "Full info here: ://t/led1LLNj3M\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "examine_topic(3, model) # Examine specific topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2aaf6211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_9a2a8_row0_col0, #T_9a2a8_row0_col1, #T_9a2a8_row0_col2, #T_9a2a8_row0_col3, #T_9a2a8_row0_col4, #T_9a2a8_row0_col5, #T_9a2a8_row0_col6, #T_9a2a8_row0_col7, #T_9a2a8_row0_col8 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_9a2a8_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >creation_time</th>\n",
       "      <th class=\"col_heading level0 col1\" >user</th>\n",
       "      <th class=\"col_heading level0 col2\" >tweet</th>\n",
       "      <th class=\"col_heading level0 col3\" >retweets</th>\n",
       "      <th class=\"col_heading level0 col4\" >favorites</th>\n",
       "      <th class=\"col_heading level0 col5\" >followers</th>\n",
       "      <th class=\"col_heading level0 col6\" >lists</th>\n",
       "      <th class=\"col_heading level0 col7\" >topic</th>\n",
       "      <th class=\"col_heading level0 col8\" >confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9a2a8_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_9a2a8_row0_col0\" class=\"data row0 col0\" >2022-09-26 20:14:17+00:00</td>\n",
       "      <td id=\"T_9a2a8_row0_col1\" class=\"data row0 col1\" >GregL_Intel</td>\n",
       "      <td id=\"T_9a2a8_row0_col2\" class=\"data row0 col2\" >I am looking forward to Intel Fellow @brendangregg  joining me during my keynote at Intel Innovation. Hear directly from a globally recognized expert in computing performance and eBPF as well as other industry experts. https://t.co/cq9gt1SuPx #IntelON</td>\n",
       "      <td id=\"T_9a2a8_row0_col3\" class=\"data row0 col3\" >10</td>\n",
       "      <td id=\"T_9a2a8_row0_col4\" class=\"data row0 col4\" >54</td>\n",
       "      <td id=\"T_9a2a8_row0_col5\" class=\"data row0 col5\" >4123</td>\n",
       "      <td id=\"T_9a2a8_row0_col6\" class=\"data row0 col6\" >41</td>\n",
       "      <td id=\"T_9a2a8_row0_col7\" class=\"data row0 col7\" >25</td>\n",
       "      <td id=\"T_9a2a8_row0_col8\" class=\"data row0 col8\" >0.484086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x11a497f1970>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.4840864], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([['webinar', 'ai', 'deepmind', 'kaggle', 'openai', 'attending',\n",
       "        'meeting', 'conference', 'technologies', 'robotics', 'discussed',\n",
       "        'panel', 'session', 'meetings', 'mit', 'vr', 'collaboration',\n",
       "        'talks', 'joining', 'iot', 'introducing', 'institute',\n",
       "        'neuroscience', 'conferences', 'neural', 'ml', 'lab', 'coursera',\n",
       "        'implications', 'invited', 'speakers', 'participate',\n",
       "        'autonomous', 'computational', 'offline', 'announced', 'virtual',\n",
       "        'generative', 'discussing', 'guest', 'speaker', 'cohort',\n",
       "        'sessions', 'cybersecurity', 'workshop', 'based', 'met',\n",
       "        'joined', 'discussions', 'third']], dtype='<U15')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweet_topic_lookup(0, model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ecefe90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e40d5_row0_col0, #T_e40d5_row0_col1, #T_e40d5_row0_col2, #T_e40d5_row0_col3, #T_e40d5_row0_col4, #T_e40d5_row0_col5, #T_e40d5_row0_col6, #T_e40d5_row0_col7, #T_e40d5_row0_col8, #T_e40d5_row0_col9, #T_e40d5_row0_col10, #T_e40d5_row1_col0, #T_e40d5_row1_col1, #T_e40d5_row1_col2, #T_e40d5_row1_col3, #T_e40d5_row1_col4, #T_e40d5_row1_col5, #T_e40d5_row1_col6, #T_e40d5_row1_col7, #T_e40d5_row1_col8, #T_e40d5_row1_col9, #T_e40d5_row1_col10, #T_e40d5_row2_col0, #T_e40d5_row2_col1, #T_e40d5_row2_col2, #T_e40d5_row2_col3, #T_e40d5_row2_col4, #T_e40d5_row2_col5, #T_e40d5_row2_col6, #T_e40d5_row2_col7, #T_e40d5_row2_col8, #T_e40d5_row2_col9, #T_e40d5_row2_col10, #T_e40d5_row3_col0, #T_e40d5_row3_col1, #T_e40d5_row3_col2, #T_e40d5_row3_col3, #T_e40d5_row3_col4, #T_e40d5_row3_col5, #T_e40d5_row3_col6, #T_e40d5_row3_col7, #T_e40d5_row3_col8, #T_e40d5_row3_col9, #T_e40d5_row3_col10, #T_e40d5_row4_col0, #T_e40d5_row4_col1, #T_e40d5_row4_col2, #T_e40d5_row4_col3, #T_e40d5_row4_col4, #T_e40d5_row4_col5, #T_e40d5_row4_col6, #T_e40d5_row4_col7, #T_e40d5_row4_col8, #T_e40d5_row4_col9, #T_e40d5_row4_col10 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e40d5_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >creation_time</th>\n",
       "      <th class=\"col_heading level0 col1\" >user</th>\n",
       "      <th class=\"col_heading level0 col2\" >tweet</th>\n",
       "      <th class=\"col_heading level0 col3\" >retweets</th>\n",
       "      <th class=\"col_heading level0 col4\" >favorites</th>\n",
       "      <th class=\"col_heading level0 col5\" >followers</th>\n",
       "      <th class=\"col_heading level0 col6\" >lists</th>\n",
       "      <th class=\"col_heading level0 col7\" >viral</th>\n",
       "      <th class=\"col_heading level0 col8\" >virality</th>\n",
       "      <th class=\"col_heading level0 col9\" >topic</th>\n",
       "      <th class=\"col_heading level0 col10\" >confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e40d5_level0_row0\" class=\"row_heading level0 row0\" >14577</th>\n",
       "      <td id=\"T_e40d5_row0_col0\" class=\"data row0 col0\" >2021-10-09 19:18:19+00:00</td>\n",
       "      <td id=\"T_e40d5_row0_col1\" class=\"data row0 col1\" >RomeoStevens76</td>\n",
       "      <td id=\"T_e40d5_row0_col2\" class=\"data row0 col2\" >(source unknown) https://t.co/cdJ25qAEIE</td>\n",
       "      <td id=\"T_e40d5_row0_col3\" class=\"data row0 col3\" >128</td>\n",
       "      <td id=\"T_e40d5_row0_col4\" class=\"data row0 col4\" >712</td>\n",
       "      <td id=\"T_e40d5_row0_col5\" class=\"data row0 col5\" >4104</td>\n",
       "      <td id=\"T_e40d5_row0_col6\" class=\"data row0 col6\" >82</td>\n",
       "      <td id=\"T_e40d5_row0_col7\" class=\"data row0 col7\" >1</td>\n",
       "      <td id=\"T_e40d5_row0_col8\" class=\"data row0 col8\" >0.001975</td>\n",
       "      <td id=\"T_e40d5_row0_col9\" class=\"data row0 col9\" >70</td>\n",
       "      <td id=\"T_e40d5_row0_col10\" class=\"data row0 col10\" >0.279908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e40d5_level0_row1\" class=\"row_heading level0 row1\" >17158</th>\n",
       "      <td id=\"T_e40d5_row1_col0\" class=\"data row1 col0\" >2022-05-22 15:58:20+00:00</td>\n",
       "      <td id=\"T_e40d5_row1_col1\" class=\"data row1 col1\" >MCMCD_</td>\n",
       "      <td id=\"T_e40d5_row1_col2\" class=\"data row1 col2\" >other coal pieces https://t.co/hJmMZzhwvv</td>\n",
       "      <td id=\"T_e40d5_row1_col3\" class=\"data row1 col3\" >25</td>\n",
       "      <td id=\"T_e40d5_row1_col4\" class=\"data row1 col4\" >367</td>\n",
       "      <td id=\"T_e40d5_row1_col5\" class=\"data row1 col5\" >3499</td>\n",
       "      <td id=\"T_e40d5_row1_col6\" class=\"data row1 col6\" >28</td>\n",
       "      <td id=\"T_e40d5_row1_col7\" class=\"data row1 col7\" >1</td>\n",
       "      <td id=\"T_e40d5_row1_col8\" class=\"data row1 col8\" >0.000452</td>\n",
       "      <td id=\"T_e40d5_row1_col9\" class=\"data row1 col9\" >70</td>\n",
       "      <td id=\"T_e40d5_row1_col10\" class=\"data row1 col10\" >0.147615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e40d5_level0_row2\" class=\"row_heading level0 row2\" >14730</th>\n",
       "      <td id=\"T_e40d5_row2_col0\" class=\"data row2 col0\" >2022-02-17 09:22:35+00:00</td>\n",
       "      <td id=\"T_e40d5_row2_col1\" class=\"data row2 col1\" >smallkewlhuman</td>\n",
       "      <td id=\"T_e40d5_row2_col2\" class=\"data row2 col2\" >@PrimeOgHarris @zaharoorin rest in peace sweet antwain &lt;3</td>\n",
       "      <td id=\"T_e40d5_row2_col3\" class=\"data row2 col3\" >18</td>\n",
       "      <td id=\"T_e40d5_row2_col4\" class=\"data row2 col4\" >1818</td>\n",
       "      <td id=\"T_e40d5_row2_col5\" class=\"data row2 col5\" >2516</td>\n",
       "      <td id=\"T_e40d5_row2_col6\" class=\"data row2 col6\" >20</td>\n",
       "      <td id=\"T_e40d5_row2_col7\" class=\"data row2 col7\" >1</td>\n",
       "      <td id=\"T_e40d5_row2_col8\" class=\"data row2 col8\" >0.000453</td>\n",
       "      <td id=\"T_e40d5_row2_col9\" class=\"data row2 col9\" >70</td>\n",
       "      <td id=\"T_e40d5_row2_col10\" class=\"data row2 col10\" >0.338763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e40d5_level0_row3\" class=\"row_heading level0 row3\" >17323</th>\n",
       "      <td id=\"T_e40d5_row3_col0\" class=\"data row3 col0\" >2021-08-24 23:35:05+00:00</td>\n",
       "      <td id=\"T_e40d5_row3_col1\" class=\"data row3 col1\" >metaphorician</td>\n",
       "      <td id=\"T_e40d5_row3_col2\" class=\"data row3 col2\" >Earthling 😵 @ziyatong https://t.co/xngI3On2jh</td>\n",
       "      <td id=\"T_e40d5_row3_col3\" class=\"data row3 col3\" >9</td>\n",
       "      <td id=\"T_e40d5_row3_col4\" class=\"data row3 col4\" >58</td>\n",
       "      <td id=\"T_e40d5_row3_col5\" class=\"data row3 col5\" >3063</td>\n",
       "      <td id=\"T_e40d5_row3_col6\" class=\"data row3 col6\" >55</td>\n",
       "      <td id=\"T_e40d5_row3_col7\" class=\"data row3 col7\" >1</td>\n",
       "      <td id=\"T_e40d5_row3_col8\" class=\"data row3 col8\" >0.000186</td>\n",
       "      <td id=\"T_e40d5_row3_col9\" class=\"data row3 col9\" >70</td>\n",
       "      <td id=\"T_e40d5_row3_col10\" class=\"data row3 col10\" >0.367412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e40d5_level0_row4\" class=\"row_heading level0 row4\" >16607</th>\n",
       "      <td id=\"T_e40d5_row4_col0\" class=\"data row4 col0\" >2021-08-18 17:16:41+00:00</td>\n",
       "      <td id=\"T_e40d5_row4_col1\" class=\"data row4 col1\" >CharlesCrooks</td>\n",
       "      <td id=\"T_e40d5_row4_col2\" class=\"data row4 col2\" >@ReturnofR “It Makes No Difference” by The Band. https://t.co/sapjCcxHrF</td>\n",
       "      <td id=\"T_e40d5_row4_col3\" class=\"data row4 col3\" >8</td>\n",
       "      <td id=\"T_e40d5_row4_col4\" class=\"data row4 col4\" >16</td>\n",
       "      <td id=\"T_e40d5_row4_col5\" class=\"data row4 col5\" >351</td>\n",
       "      <td id=\"T_e40d5_row4_col6\" class=\"data row4 col6\" >1</td>\n",
       "      <td id=\"T_e40d5_row4_col7\" class=\"data row4 col7\" >1</td>\n",
       "      <td id=\"T_e40d5_row4_col8\" class=\"data row4 col8\" >0.001443</td>\n",
       "      <td id=\"T_e40d5_row4_col9\" class=\"data row4 col9\" >70</td>\n",
       "      <td id=\"T_e40d5_row4_col10\" class=\"data row4 col10\" >0.395987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1b5e6dcb100>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topic top tweet Lookup\n",
    "data.loc[data['topic'] == 70].sort_values('retweets', ascending=False).head().style.set_properties(**{'text-align': 'left'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38e6c8",
   "metadata": {},
   "source": [
    "### Preparing data for export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe8b7922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for loop - look up each document and append the topic ID to the original dataframe\n",
    "topic_ids = []\n",
    "topic_confidence = []\n",
    "for doc in model.document_ids: # model.document_ids # List of all document IDs\n",
    "    topic_confidence.append(model.get_documents_topics([doc])[1][0]) # Grabbing Confidence\n",
    "    topic_ids.append(model.get_documents_topics([doc])[0][0]) # Grabbing Topic ID\n",
    "data['topic'] = topic_ids\n",
    "data['confidence'] = topic_confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bbe2e6",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf797c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['topic'].to_csv('topics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e8b89",
   "metadata": {},
   "source": [
    ":)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
